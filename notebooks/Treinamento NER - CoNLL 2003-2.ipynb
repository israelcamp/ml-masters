{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers.tokenization_bert import BertTokenizer\n",
    "from transformers.modeling_bert import BertForTokenClassification, BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpack.datasets.conll2003 import get_conll2003, get_conll2003_features, convert_examples_to_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, labels = get_conll2003('../datasets/CoNLL2003/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: 'O',\n",
       " 2: 'B-MISC',\n",
       " 3: 'I-MISC',\n",
       " 4: 'B-PER',\n",
       " 5: 'I-PER',\n",
       " 6: 'B-ORG',\n",
       " 7: 'I-ORG',\n",
       " 8: 'B-LOC',\n",
       " 9: 'I-LOC',\n",
       " 10: '[CLS]',\n",
       " 11: '[SEP]',\n",
       " 12: 'X'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {i:l for i, l in enumerate(labels, 0)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = convert_examples_to_features(examples['train'], labels, 128, tokenizer, sep_tag='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_valid = convert_examples_to_features(examples['valid'], labels, 128, tokenizer, sep_tag='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = get_conll2003_features(examples, labels, 128, tokenizer, sep_tag='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "ex, feat = examples['train'][idx], features_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]                [CLS]  0\n",
      "spanish              B-MISC 1\n",
      "farm                 O      1\n",
      "minister             O      1\n",
      "loyola               B-PER  1\n",
      "de                   I-PER  1\n",
      "pal                  I-PER  1\n",
      "##acio               X      0\n",
      "had                  O      1\n",
      "earlier              O      1\n",
      "accused              O      1\n",
      "fis                  B-PER  1\n",
      "##ch                 X      0\n",
      "##ler                X      0\n",
      "at                   O      1\n",
      "an                   O      1\n",
      "eu                   B-ORG  1\n",
      "farm                 O      1\n",
      "ministers            O      1\n",
      "'                    O      1\n",
      "meeting              O      1\n",
      "of                   O      1\n",
      "causing              O      1\n",
      "un                   O      1\n",
      "##just               X      0\n",
      "##ified              X      0\n",
      "alarm                O      1\n",
      "through              O      1\n",
      "\"                    O      1\n",
      "dangerous            O      1\n",
      "general              O      1\n",
      "##isation            X      0\n",
      ".                    O      1\n",
      "\"                    O      1\n",
      "[SEP]                [SEP]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n",
      "[PAD]                [PAD]  0\n"
     ]
    }
   ],
   "source": [
    "for token, label_id, mask in zip(tokenizer.convert_ids_to_tokens(feat.input_ids), feat.label_id, feat.label_mask):\n",
    "    print(f'{token:20} {label_map[label_id]:6} {mask}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = self.features[idx]\n",
    "        return torch.tensor(feat.input_ids), torch.tensor(feat.input_mask), \\\n",
    "                torch.tensor(feat.label_id), torch.tensor(feat.label_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = NERDataset(features_train)\n",
    "ds_valid = NERDataset(features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=1, pin_memory=True, shuffle=True,  num_workers=0)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=1, pin_memory=True, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_ids, input_mask, label_ids, label_mask = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids.shape, input_mask.shape, label_ids.shape, label_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _optimizer_ckp_path(ckp_path):\n",
    "    fmt = ckp_path.split('/')[-1].split('.')[-1]\n",
    "    optim_path = ckp_path.replace(f'.{fmt}', f'_optimizer.{fmt}')\n",
    "    return optim_path\n",
    "\n",
    "\n",
    "def _scheduler_ckp_path(ckp_path):\n",
    "    fmt = ckp_path.split('/')[-1].split('.')[-1]\n",
    "    sched_path = ckp_path.replace(f'.{fmt}', f'_lrscheduler.{fmt}')\n",
    "    return sched_path\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, ckp_path, scheduler=None):\n",
    "    torch.save(model.state_dict(), ckp_path)\n",
    "    # saving the optimizer\n",
    "    optim_path = _optimizer_ckp_path(ckp_path)\n",
    "    torch.save(optimizer.state_dict(), optim_path)\n",
    "    if scheduler:\n",
    "        sched_path = _scheduler_ckp_path(ckp_path)\n",
    "        torch.save(scheduler.state_dict(), optim_path)\n",
    "    print('Saved new checkpoint', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(*tensors, device='cpu'):\n",
    "    return [\n",
    "        t.to(device) for t in tensors\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap(input_ids, input_mask, label_ids, label_mask, active_logits, active_labels):\n",
    "    start = 0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    for i in range(input_ids.shape[0]):\n",
    "        in_ids, in_mask, l_ids, l_mask = input_ids[i], input_mask[i], label_ids[i] - 1, label_mask[i]\n",
    "        in_mask =  in_mask[in_mask == 1][1:-1] # tira cls e sep\n",
    "        l_mask = l_mask[1:len(in_mask) + 1]\n",
    "\n",
    "        end = len(l_mask[l_mask == 1])\n",
    "        preds_labels = active_logits[start:end].argmax(1)\n",
    "        true_labels  = active_labels[start:end]\n",
    "\n",
    "        pred_ents, true_ents, count, count_true = [], [], 0, 0\n",
    "        for j, lm in enumerate(l_mask):\n",
    "            if lm == 1:\n",
    "                pred_ents.append(LABELS[preds_labels[count].item()])\n",
    "                count += 1\n",
    "                #true\n",
    "                true_ents.append(LABELS[true_labels[count_true].item()])\n",
    "                count_true += 1\n",
    "            else:\n",
    "                pl = LABELS[preds_labels[count-1].item()]\n",
    "                if pl.startswith('B'):\n",
    "                    pl = 'I-'+pl.split('-')[-1]\n",
    "                pred_ents.append(pl)\n",
    "                #true\n",
    "                pl = LABELS[true_labels[count_true-1].item()]\n",
    "                if pl.startswith('B'):\n",
    "                    pl = 'I-'+pl.split('-')[-1]\n",
    "                true_ents.append(pl)\n",
    "\n",
    "        preds.append(pred_ents)\n",
    "        trues.append(true_ents)\n",
    "\n",
    "        start = end\n",
    "    return trues, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, dataloader):\n",
    "    model.eval()\n",
    "    losses, accs = [], []\n",
    "    y_trues, y_preds = [], []\n",
    "    for input_ids, input_mask, label_ids, label_mask in tqdm(dataloader, desc='Evaluating', leave=False):\n",
    "        input_ids, input_mask, label_ids, label_mask = to_device(input_ids, input_mask, label_ids,\n",
    "                                                                 label_mask, device=device)\n",
    "        with torch.no_grad():\n",
    "            loss, active_logits, active_labels = model(\n",
    "                input_ids, input_mask, label_ids, label_mask)\n",
    "            \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        active_logits = active_logits.argmax(dim=1).cpu().numpy()\n",
    "        active_labels = active_labels.cpu().numpy()\n",
    "        accs = (1 * (active_logits == active_labels)).tolist()\n",
    "        \n",
    "        # transforming\n",
    "        ts, ps = remap(input_ids, input_mask, label_ids, label_mask, active_logits, active_labels)\n",
    "        y_preds += ps\n",
    "        y_trues += ts\n",
    "        \n",
    "#     print(y_preds, y_trues)\n",
    "    print(classification_report(y_trues, y_preds))\n",
    "            \n",
    "    return np.array(losses).mean(), np.array(accs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do this with the X tag for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = [\n",
    "    l for l in labels if l not in ['[PAD]', '[CLS]', '[SEP]', 'X']\n",
    "]\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForNERClassification(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, len(LABELS))\n",
    "        \n",
    "    @staticmethod\n",
    "    def sum_last_4_layers(sequence_outputs):\n",
    "        \"\"\"Sums the last 4 hidden representations of a sequence output of BERT.\n",
    "        Args:\n",
    "        -----\n",
    "        sequence_output: Tuple of tensors of shape (batch, seq_length, hidden_size).\n",
    "            For BERT base, the Tuple has length 13.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        summed_layers: Tensor of shape (batch, seq_length, hidden_size)\n",
    "        \"\"\"\n",
    "        last_layers = sequence_outputs[-4:]\n",
    "        return torch.stack(last_layers, dim=0).sum(dim=0)\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, label_ids, label_mask):\n",
    "        \n",
    "        _, _, hidden_states = self.bert(input_ids, attention_mask=input_mask)\n",
    "        \n",
    "        out = self.sum_last_4_layers(hidden_states)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = self.classifier(out)\n",
    "        # take the active logits\n",
    "        label_mask = label_mask.view(-1)\n",
    "        active_logits = out.view(-1, len(LABELS))[label_mask == 1]\n",
    "        \n",
    "        # take the active labels\n",
    "        active_labels = label_ids.view(-1)[label_mask == 1] - 1 # remove one because of the [PAD] being the 0\n",
    "        \n",
    "        # calc the loss\n",
    "        loss = nn.CrossEntropyLoss(torch.tensor([0.5] + 8 *[1.0]).to(device))(active_logits, active_labels)\n",
    "        \n",
    "        return loss, active_logits, active_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNERClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForNERClassification()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('nerbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    device = device\n",
    "    fp16 = True\n",
    "    num_epochs = 10\n",
    "    ckp_path = 'bertner.ckp'\n",
    "    grad_steps = 1\n",
    "    max_grad_norm = 1.\n",
    "    load_state_dict = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.load_state_dict:\n",
    "    if os.path.exists(args.ckp_path):\n",
    "        print(model.load_state_dict(torch.load(args.ckp_path)))\n",
    "#     if os.path.exists(args.ckp_path.replace('.ckp', '_optimizer.ckp')):\n",
    "#         optimizer.load_state_dict(torch.load(args.ckp_path.replace('.ckp', '_optimizer.ckp'), map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "best_acc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "if args.fp16:\n",
    "    try:\n",
    "        from apex import amp\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "    model, optimizer = amp.initialize(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfeca62bb9b4b7eae27b1496d300d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs', max=10, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=439), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=439, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.57      0.51      0.54      7140\n",
      "      ORG       0.45      0.27      0.34      6321\n",
      "      PER       0.10      0.09      0.10      6600\n",
      "     MISC       0.52      0.48      0.50      3438\n",
      "\n",
      "micro avg       0.39      0.32      0.35     23499\n",
      "macro avg       0.40      0.32      0.35     23499\n",
      "\n",
      "---Valid\n",
      "Loss 0.48014770086392217\n",
      "Acc 0.8714859437751004\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=439), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=439, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.57      0.51      0.54      7140\n",
      "      ORG       0.41      0.36      0.38      6321\n",
      "      PER       0.15      0.21      0.17      6600\n",
      "     MISC       0.63      0.49      0.55      3438\n",
      "\n",
      "micro avg       0.37      0.38      0.38     23499\n",
      "macro avg       0.42      0.38      0.39     23499\n",
      "\n",
      "---Valid\n",
      "Loss 0.2934071801402362\n",
      "Acc 0.9397590361445783\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=439), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=439, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.64      0.51      0.57      7140\n",
      "      ORG       0.41      0.39      0.40      6321\n",
      "      PER       0.15      0.23      0.18      6600\n",
      "     MISC       0.65      0.50      0.56      3438\n",
      "\n",
      "micro avg       0.39      0.40      0.39     23499\n",
      "macro avg       0.44      0.40      0.41     23499\n",
      "\n",
      "---Valid\n",
      "Loss 0.2101376980934878\n",
      "Acc 0.9678714859437751\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=439), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=439, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      LOC       0.63      0.53      0.58      7140\n",
      "      ORG       0.43      0.40      0.41      6321\n",
      "      PER       0.16      0.24      0.19      6600\n",
      "     MISC       0.62      0.54      0.58      3438\n",
      "\n",
      "micro avg       0.39      0.42      0.40     23499\n",
      "macro avg       0.44      0.42      0.42     23499\n",
      "\n",
      "---Valid\n",
      "Loss 0.16489035021144433\n",
      "Acc 0.9959839357429718\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5fba77446b490796b5737ec0bf9de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=439), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2f569d08fe34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 torch.nn.utils.clip_grad_norm_(\n\u001b[1;32m     17\u001b[0m                     amp.master_params(optimizer), args.max_grad_norm)\n",
      "\u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in tqdm(range(args.num_epochs), desc='Epochs'):\n",
    "    model.train()\n",
    "    if ep > -1:\n",
    "        for step, (input_ids, input_mask, label_ids, label_mask) in tqdm(enumerate(dl_train), leave=False, total=len(dl_train)):\n",
    "            input_ids, input_mask, label_ids, label_mask = to_device(input_ids, input_mask, label_ids,\n",
    "                                                                     label_mask, device=device)\n",
    "\n",
    "            loss, _, _ = model(input_ids, input_mask, label_ids, label_mask)\n",
    "\n",
    "            if args.grad_steps > 1:\n",
    "                loss = loss / args.grad_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            if (step + 1) % args.grad_steps == 0 or (step + 1) == len(dl_train):\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "            if writer:\n",
    "                writer.add_scalar('loss/train', loss, n_iter)\n",
    "            n_iter += 1\n",
    "\n",
    "    # evaluate\n",
    "    valid_loss, valid_acc = evaluate_fn(model, dl_valid)\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar('loss/valid', valid_loss, ep)\n",
    "        writer.add_scalar('acc/valid', valid_acc, ep)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(name, param, ep)\n",
    "\n",
    "    print(f'---Valid\\nLoss {valid_loss}\\nAcc {valid_acc}', flush=True)\n",
    "\n",
    "    if best_acc is None:\n",
    "        best_acc = valid_acc\n",
    "        save_model(model, optimizer, args.ckp_path, scheduler=scheduler)\n",
    "    else:\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            save_model(model, optimizer, args.ckp_path,\n",
    "                       scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bertenv]",
   "language": "python",
   "name": "conda-env-bertenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
