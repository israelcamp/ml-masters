{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "dataset_name = 'census-income'\n",
    "out = Path(os.getcwd().rsplit(\"/\",  1)[0]+'/data/'+dataset_name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file...\n"
     ]
    }
   ],
   "source": [
    "out.parent.mkdir(parents=True, exist_ok=True)\n",
    "if out.exists():\n",
    "    print(\"File already exists.\")\n",
    "else:\n",
    "    print(\"Downloading file...\")\n",
    "    wget.download(url, out.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(out)\n",
    "target = ' <=50K'\n",
    "if \"Set\" not in train.columns:\n",
    "    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(train.shape[0],))\n",
    "\n",
    "train_indices = train[train.Set==\"train\"].index\n",
    "valid_indices = train[train.Set==\"valid\"].index\n",
    "test_indices = train[train.Set==\"test\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " State-gov 9\n",
      " Bachelors 16\n",
      " Never-married 7\n",
      " Adm-clerical 15\n",
      " Not-in-family 6\n",
      " White 5\n",
      " Male 2\n",
      " United-States 42\n",
      " <=50K 2\n",
      "Set 3\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in train.columns[train.dtypes == object]:\n",
    "    print(col, train[col].nunique())\n",
    "    l_enc = LabelEncoder()\n",
    "    train[col] = train[col].fillna(\"VV_likely\")\n",
    "    train[col] = l_enc.fit_transform(train[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "for col in train.columns[train.dtypes == 'float64']:\n",
    "    train.fillna(train.loc[train_indices, col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>39</th>\n",
       "      <th>State-gov</th>\n",
       "      <th>77516</th>\n",
       "      <th>Bachelors</th>\n",
       "      <th>13</th>\n",
       "      <th>Never-married</th>\n",
       "      <th>Adm-clerical</th>\n",
       "      <th>Not-in-family</th>\n",
       "      <th>White</th>\n",
       "      <th>Male</th>\n",
       "      <th>2174</th>\n",
       "      <th>0</th>\n",
       "      <th>40</th>\n",
       "      <th>United-States</th>\n",
       "      <th>&lt;=50K</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>284582</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   39   State-gov   77516   Bachelors   13   Never-married   Adm-clerical  \\\n",
       "0  50           6   83311           9   13               2              4   \n",
       "1  38           4  215646          11    9               0              6   \n",
       "2  53           4  234721           1    7               2              6   \n",
       "3  28           4  338409           9   13               2             10   \n",
       "4  37           4  284582          12   14               2              4   \n",
       "\n",
       "    Not-in-family   White   Male   2174   0   40   United-States   <=50K  Set  \n",
       "0               0       4      1      0   0   13              39       0    1  \n",
       "1               1       4      1      0   0   40              39       0    1  \n",
       "2               0       2      1      0   0   40              39       0    1  \n",
       "3               5       2      0      0   0   40               5       0    1  \n",
       "4               5       4      0      0   0   40              39       0    1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns.remove('Set')\n",
    "categorical_columns.remove(' <=50K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns = [s for s in all_columns if s not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_columns.remove('Set')\n",
    "cont_columns.remove(' <=50K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = ' <=50K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['39', ' 77516', ' 13', ' 2174', ' 0', ' 40'],\n",
       " [' State-gov',\n",
       "  ' Bachelors',\n",
       "  ' Never-married',\n",
       "  ' Adm-clerical',\n",
       "  ' Not-in-family',\n",
       "  ' White',\n",
       "  ' Male',\n",
       "  ' United-States'],\n",
       " [' <=50K'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_columns, categorical_columns, target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cont_columns + categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_cat_cols = len(categorical_columns)\n",
    "len_cont_cols = len(cont_columns)\n",
    "cat_emb_dim=[2, 2, 3, 2, 2, 3, 2, 2]\n",
    "len(cat_emb_dim), len_cat_cols, len_cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 16, 7, 15, 6, 5, 2, 42]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(train[target].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[features].values[train_indices]\n",
    "y_train = train[target].values[train_indices]\n",
    "\n",
    "X_valid = train[features].values[valid_indices]\n",
    "y_valid = train[target].values[valid_indices]\n",
    "\n",
    "X_test = train[features].values[test_indices]\n",
    "y_test = train[target].values[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train))\n",
    "ds_valid = TensorDataset(torch.tensor(X_valid).float(), torch.tensor(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=4)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Other possible implementations:\n",
    "https://github.com/KrisKorrel/sparsemax-pytorch/blob/master/sparsemax.py\n",
    "https://github.com/msobroza/SparsemaxPytorch/blob/master/mnist/sparsemax.py\n",
    "https://github.com/vene/sparse-structured-attention/blob/master/pytorch/torchsparseattn/sparsemax.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py\n",
    "def _make_ix_like(input, dim=0):\n",
    "    d = input.size(dim)\n",
    "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
    "    view = [1] * input.dim()\n",
    "    view[0] = -1\n",
    "    return rho.view(view).transpose(0, dim)\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "    \"\"\"\n",
    "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
    "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
    "    By Ben Peters and Vlad Niculae\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, dim=-1):\n",
    "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
    "        Parameters:\n",
    "            input (Tensor): any shape\n",
    "            dim: dimension along which to apply sparsemax\n",
    "        Returns:\n",
    "            output (Tensor): same shape as input\n",
    "        \"\"\"\n",
    "        ctx.dim = dim\n",
    "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
    "        input -= max_val  # same numerical stability trick as for softmax\n",
    "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
    "        output = torch.clamp(input - tau, min=0)\n",
    "        ctx.save_for_backward(supp_size, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output = ctx.saved_tensors\n",
    "        dim = ctx.dim\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n",
    "        v_hat = v_hat.unsqueeze(dim)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
    "        return grad_input, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_and_support(input, dim=-1):\n",
    "        \"\"\"Sparsemax building block: compute the threshold\n",
    "        Args:\n",
    "            input: any dimension\n",
    "            dim: dimension along which to apply the sparsemax\n",
    "        Returns:\n",
    "            the threshold value\n",
    "        \"\"\"\n",
    "\n",
    "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
    "        input_cumsum = input_srt.cumsum(dim) - 1\n",
    "        rhos = _make_ix_like(input, dim)\n",
    "        support = rhos * input_srt > input_cumsum\n",
    "\n",
    "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
    "        tau = input_cumsum.gather(dim, support_size - 1)\n",
    "        tau /= support_size.to(input.dtype)\n",
    "        return tau, support_size\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return sparsemax(input, self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x[:, :self.units] * torch.sigmoid(x[:, self.units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bn_momentum, previous_transformer=None):\n",
    "        super().__init__()\n",
    "        self.block1 = self.block(in_features, out_features, bn_momentum)\n",
    "        self.block2 = self.block(out_features, out_features, bn_momentum)\n",
    "        self.previous_transformer = previous_transformer\n",
    "\n",
    "    def block(self, in_features, out_features, bn_momentum):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features * 2, bias=False),\n",
    "            nn.BatchNorm1d(out_features * 2, momentum=bn_momentum),\n",
    "            GLU(out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.previous_transformer is not None:\n",
    "            x = self.previous_transformer(x)\n",
    "\n",
    "        o1 = self.block1(x)\n",
    "\n",
    "        if self.previous_transformer is not None:\n",
    "            o1 = (o1 + x) * math.sqrt(0.5)\n",
    "\n",
    "        o2 = self.block2(o1)\n",
    "        return (o2 + o1) * math.sqrt(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, comp_size, n_features, relaxation_factor, epsilon):\n",
    "        super().__init__()\n",
    "        self.relaxation_factor = relaxation_factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.dense = nn.Linear(comp_size, n_features)\n",
    "        self.bn = nn.BatchNorm1d(n_features)\n",
    "        self.sparsemax = Sparsemax()\n",
    "\n",
    "    def forward(self, features_for_coef, complemantary_aggregated_mask_values):\n",
    "        mask_values = self.dense(features_for_coef)\n",
    "        mask_values = self.bn(mask_values)\n",
    "        mask_values =  mask_values * complemantary_aggregated_mask_values\n",
    "        mask_values = self.sparsemax(mask_values)\n",
    "\n",
    "        complemantary_aggregated_mask_values = complemantary_aggregated_mask_values * (\n",
    "            self.relaxation_factor - mask_values)\n",
    "        entropy = torch.sum(-mask_values *\n",
    "                            torch.log(mask_values + self.epsilon), dim=1)\n",
    "        entropy = entropy.mean()\n",
    "        return mask_values, complemantary_aggregated_mask_values, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, hidden_size, output_dim, decision_steps=2,\n",
    "                 relaxation_factor=1, epsilon=1e-4, bn_momentum=0.7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.n_features = n_features\n",
    "        self.decision_steps = decision_steps\n",
    "        self.bn_momentum = bn_momentum\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(n_features, momentum=self.bn_momentum)\n",
    "\n",
    "        # feature transformer shared accross steps\n",
    "        self.shared_transformer = FeatureTransformer(n_features, hidden_size, self.bn_momentum)\n",
    "\n",
    "        # step dependent feature transformer\n",
    "        self.step_transformers = nn.ModuleList([\n",
    "            FeatureTransformer(hidden_size, hidden_size, self.bn_momentum,\n",
    "                               self.shared_transformer)\n",
    "            for _ in range(decision_steps)\n",
    "        ])\n",
    "\n",
    "        self.attentive_transformers = nn.ModuleList([\n",
    "            AttentiveTransformer(\n",
    "                self.hidden_size-self.output_dim, self.n_features, relaxation_factor, epsilon)\n",
    "            for _ in range(decision_steps - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, output_masks=False):\n",
    "        x = self.bn(x)  # applies batch norm on the input\n",
    "        batch_size = x.shape[0]\n",
    "        output_aggregated = torch.zeros(batch_size, self.output_dim).to(device)\n",
    "        mask_values = torch.zeros(batch_size, self.n_features).to(device)\n",
    "        aggregated_mask_values = torch.zeros(\n",
    "            batch_size, self.n_features).to(device)\n",
    "        complemantary_aggregated_mask_values = torch.ones(\n",
    "            batch_size, self.n_features).to(device)\n",
    "        total_entropy = 0\n",
    "\n",
    "        mask_values_per_step = []\n",
    "\n",
    "        masked_features = x\n",
    "\n",
    "        for i, step_transformer in enumerate(self.step_transformers):\n",
    "\n",
    "            o = step_transformer(masked_features)\n",
    "\n",
    "            if i > 0:\n",
    "                decision_out = F.relu(o[:, :self.output_dim])\n",
    "                output_aggregated = output_aggregated + decision_out\n",
    "\n",
    "                # for visualization\n",
    "                scale_agg = torch.sum(\n",
    "                    decision_out, dim=1, keepdim=True) / (self.decision_steps - 1)\n",
    "                aggregated_mask_values = aggregated_mask_values + mask_values * scale_agg\n",
    "\n",
    "            features_for_coef = o[:, self.output_dim:]\n",
    "\n",
    "            if i < self.decision_steps - 1:\n",
    "                attentive_transformer = self.attentive_transformers[i]\n",
    "                mask_values, complemantary_aggregated_mask_values, entropy = attentive_transformer(\n",
    "                    features_for_coef, complemantary_aggregated_mask_values)\n",
    "                total_entropy = total_entropy + \\\n",
    "                    entropy / (self.decision_steps - 1)\n",
    "\n",
    "                masked_features = mask_values * x\n",
    "\n",
    "                if output_masks:\n",
    "                    mask_values_per_step.append(mask_values.detach().cpu())\n",
    "\n",
    "        outputs = (output_aggregated, total_entropy)\n",
    "        if output_masks:\n",
    "            outputs = outputs + (aggregated_mask_values, mask_values_per_step)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabNetClassifier(TabNet):\n",
    "    \n",
    "    def __init__(self, output_dim, n_classes, n_cont_feats, cat_dims, cat_emb_dims, **tabnet_kwargs):\n",
    "        \n",
    "        self.n_cont_feats = n_cont_feats\n",
    "        self.total_cat_feats = sum(cat_emb_dims)\n",
    "        self.total_feats = n_cont_feats + self.total_cat_feats\n",
    "        \n",
    "        super().__init__(output_dim=output_dim,n_features=self.total_feats, **tabnet_kwargs)\n",
    "        \n",
    "        assert len(cat_dims) == len(cat_emb_dims)\n",
    "                \n",
    "\n",
    "        \n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(nc, ne) for nc, ne in zip(cat_dims, cat_emb_dims)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(output_dim, n_classes, bias=False)\n",
    "    \n",
    "    def forward(self, x, output_masks=False):\n",
    "        x_cont = x[:,:self.n_cont_feats]\n",
    "        x_cat = torch.empty(x.shape[0], self.total_cat_feats).to(x.device)\n",
    "        \n",
    "        start = 0\n",
    "        for i, emb in enumerate(self.embs):\n",
    "            xi = x[:, i + self.n_cont_feats].long()\n",
    "            xemb = emb(xi)            \n",
    "            end = start + xemb.shape[1]\n",
    "            x_cat[:, start:end] = xemb\n",
    "            start = end\n",
    "            \n",
    "        x = torch.cat((x_cont, x_cat), dim=1)\n",
    "        \n",
    "        outputs = super().forward(x, output_masks)\n",
    "        logits = self.classifier(outputs[0])\n",
    "        outputs = (logits,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features = 16\n",
    "output_dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabnet = TabNet(in_features, out_features, output_dim, decision_steps=4)\n",
    "model = TabNetClassifier(output_dim, n_classes,\n",
    "                        n_cont_feats=len_cont_cols,\n",
    "                        cat_dims=cat_dims,\n",
    "                        cat_emb_dims=cat_emb_dim,\n",
    "                        hidden_size=out_features,\n",
    "                        relaxation_factor=1.0,\n",
    "                        bn_momentum=0.1,\n",
    "                        epsilon=1e-15,\n",
    "                        decision_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabNetClassifier(\n",
       "  (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (shared_transformer): FeatureTransformer(\n",
       "    (block1): Sequential(\n",
       "      (0): Linear(in_features=24, out_features=32, bias=False)\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GLU()\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GLU()\n",
       "    )\n",
       "  )\n",
       "  (step_transformers): ModuleList(\n",
       "    (0): FeatureTransformer(\n",
       "      (block1): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (block2): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (previous_transformer): FeatureTransformer(\n",
       "        (block1): Sequential(\n",
       "          (0): Linear(in_features=24, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): FeatureTransformer(\n",
       "      (block1): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (block2): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (previous_transformer): FeatureTransformer(\n",
       "        (block1): Sequential(\n",
       "          (0): Linear(in_features=24, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): FeatureTransformer(\n",
       "      (block1): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (block2): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (previous_transformer): FeatureTransformer(\n",
       "        (block1): Sequential(\n",
       "          (0): Linear(in_features=24, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): FeatureTransformer(\n",
       "      (block1): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (block2): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (previous_transformer): FeatureTransformer(\n",
       "        (block1): Sequential(\n",
       "          (0): Linear(in_features=24, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "        (block2): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attentive_transformers): ModuleList(\n",
       "    (0): AttentiveTransformer(\n",
       "      (dense): Linear(in_features=8, out_features=24, bias=True)\n",
       "      (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (sparsemax): Sparsemax()\n",
       "    )\n",
       "    (1): AttentiveTransformer(\n",
       "      (dense): Linear(in_features=8, out_features=24, bias=True)\n",
       "      (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (sparsemax): Sparsemax()\n",
       "    )\n",
       "    (2): AttentiveTransformer(\n",
       "      (dense): Linear(in_features=8, out_features=24, bias=True)\n",
       "      (bn): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (sparsemax): Sparsemax()\n",
       "    )\n",
       "  )\n",
       "  (embs): ModuleList(\n",
       "    (0): Embedding(9, 2)\n",
       "    (1): Embedding(16, 2)\n",
       "    (2): Embedding(7, 3)\n",
       "    (3): Embedding(15, 2)\n",
       "    (4): Embedding(6, 2)\n",
       "    (5): Embedding(5, 3)\n",
       "    (6): Embedding(2, 2)\n",
       "    (7): Embedding(42, 2)\n",
       "  )\n",
       "  (classifier): Linear(in_features=8, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x.float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41090923089247483 0.7867122011797578\n",
      "0.41443082575614637 0.77708786091276\n",
      "0.41110552847385406 0.7705681465383422\n",
      "0.3624224548156445 0.8314188140329091\n",
      "0.3923909274431375 0.776156473144986\n",
      "0.39738955520666563 0.7643588947531822\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-7a67a1d57a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bertenv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bertenv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(60):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    for i, (x, y) in enumerate(dl_train):\n",
    "        \n",
    "        \n",
    "        out = model(x.float().to(device))\n",
    "        loss = loss_fn(out[0], y.long().to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "    loss_valid = 0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for x, y in dl_valid:\n",
    "        with torch.no_grad():\n",
    "            out = model(x.float().to(device))\n",
    "            loss_valid += loss_fn(out[0], y.long().to(device)).item()\n",
    "            \n",
    "        preds += out[0].argmax(-1).cpu().numpy().tolist()\n",
    "        trues += y.cpu().numpy().tolist()\n",
    "    \n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    print(loss_valid/len(dl_valid), (1.* (preds==trues)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8750)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[0].argmax(-1).cpu() == y.long()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, out_agg, _, agg_mask, mask_per_step = model(x.float().to(device), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(13.3430, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(0., device='cuda:0', grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_agg.max(), out_agg.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 24]),\n",
       " tensor(28.1767, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(0., device='cuda:0', grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_mask.shape, agg_mask.max(), agg_mask.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_per_step[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_mask = agg_mask.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_mask_norm = (agg_mask - agg_mask.min(0)) / (agg_mask.max(0) - agg_mask.min(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([128, 24]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask_per_step), mask_per_step[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(ncols=len(mask_per_step), figsize=(20,10))\n",
    "for m, ax in zip(mask_per_step, axs):\n",
    "    ax.imshow(m[:50], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4886fb3290>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask_per_step[0][:50], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3061, 0.0000, 0.2359,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.3257, 0.0000, 0.2672,  ..., 0.0000, 0.2272, 0.5230],\n",
       "        [0.0000, 0.6525, 0.0000,  ..., 0.0000, 0.1199, 0.0000],\n",
       "        ...,\n",
       "        [0.9931, 0.0000, 0.4860,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.6868, 0.0000, 0.1048,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mask_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f34307eca10>"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAD8AAAD5CAYAAACQ7u12AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAJ2klEQVR4nO2dXWwU1xXHf8fGrAnfBSc4xECo/ECo1FCR0BIesFARoNK0iObjoUJRVYgoUpHKQ0VFG/UpD/1IHioIED4qtYVCkxY3KbROKzU8NAVMzHcrgwgYSDBtwBgQwfbpw47TlZl7dz3rHX+c+5NWOzvn3jPnrzs7d2b27BlRVaxS1t8B9CdBvFWCeKuYFj+smM4isgh4FSgHtqrqy3naO+fV6upqZ7+2tjZvHMOGuWXcuHHjmqpWxfbzevUgIuXAL4AvAy3AIRHZp6qnkgS6atUqZ5/9+/d7Y6mqitUGQH19/QcuWzG7/ZNAs6qeU9VPgF3A00X4S51ixE8GLuZ8bonWDRqK+c5LzLr7vtMishJYWcR2SkYx4luAmpzPjwCXezZS1c3AZvAf8PqDYnb7Q0CtiDwqIsOB54B9fRNWOiQeeVXtEJE1wAGyU902VT3p65PJZKipqYm17dixw9lvz5493lgaGhqctvr6eqetqHleVd8G3i7GR39i+gwviLdKEG8V0+KLmup6i4iQyWRiba2trc5+dXV1Xr+vvPJKonhMj3wQb5Ug3iqmxac61XV1dXHnzp1Y27hx45z99u7d6/Xru6T1YXrkg3irBPFWMS0+1anu3r17XLx4Mda2YMECZ79nn33W63fUqFGJ4jE98kG8VYJ4q5gWX2xOznngJtAJdKjqbF/78vJyxo8fH2s7evSos9/WrVu9cZw4ccJpO3bsmNPWF/N8nape6wM/qWN6ty9WvAJ/FpEjUfrJoKLY3f4pVb0sIg8CfxGRM6r699wGuTk5ZWUDa0crKhpVvRy9XwXeJJue1rPNZlWdraqzh4x4ERkpIqO7l4GFgPuwOwApZrd/CHhTRLr9/FpV/amSA4xiEpLOAZ/vTZ+ysjJGjBgRa+vq6nL2q62t9fr1zfPeeBL1GiIE8VYJ4q1iWnzqP1TeunUr1lZZWensd/PmTa/fM2fOJIrH9MgH8VYJ4q1iWnyqU93w4cOZOnVqrO3ChQvOfosXL/b6Xb58eaJ4TI98EG+VIN4qpsUPmH9aTJgwwdnv9OnTXr/btm1z2jZt2uS0mR75IN4qQbxVgnir5J3nRWQb8BXgqqp+Llr3GWA3MA04Dzyjqh/n8zV69GjnvyNfe+01Z78xY8Z4/a5fvz7fpmMpZOR3AIt6rPs+8I6q1gLvRJ8HHXnFR2km/+2x+mlgZ7S8E/haH8eVCkm/8w+p6hWA6P1BV0MRWSkih0Xk8O3btxNurjSU/ICXm5PzwAMPlHpzvSKp+I9EpBoger/adyGlR1Lx+4AV0fIK4A99E066FDLV/QaYD0wUkRbgR8DLwG9F5FvABeAbhWwsk8kwffr0WFt5ebmzX0dHh9fvvHnzCtn8feQVr6rPO0zuP8UMEkyf4QXxVgnirZLq3dv29nYOHjwYa/vwww+d/Z544gmv3xdeeCFRPKZHPoi3ShBvFdPiU53qOjs7uX79eqztwIEDzn7Xrvn/szhz5kynrbm52WkzPfJBvFWCeKsE8VZJ/ZL23XffjbX5Llvv3bvn9fvxx3l/I43F9MgH8VYJ4q1iWnzSnJyXgG8D3aXJ10cP9/BSXV3NunXrYm0bN2509rt06ZLXb0tLi9MWVXaIJWlODsDPVfXx6DUon2iSNCdnSFDMd36NiBwTkW0iEl/waoCTVPxG4LPA48AV4KeuhrkJSe3t7Qk3VxoSiVfVj1S1U1W7gC3EFAfKaftpQlLSQp2lIpH47mSkiK8zyIoDdZM0J2e+iDxOtjDYecD9jMUcWltb2bJlS6zN91jofMUBkj7QI2lOzuuJtjbAMH2GF8RbJYi3imnxqd69vXv3LmfPno21zZgxw9lv4cKFXr9TpkxJFI/pkQ/irRLEW8W0+FSnuoqKCqqrq/M37MHOnTu99g0bNiSKx/TIB/FWCeKtYlp8qlOdj7a2Nqct3/Q4Z84cp831txYwPvJBvFWCeKuYFl/ID5U1wC+BSUAXsFlVX01SK2fSpEnOnJy1a9c6+y1aFJcV839mzZrltbsoZOQ7gO+p6gzgi8B3ROQxhkCtnEJycq6oamO0fBM4DUxmCNTK6dV3XkSmAbOA9+hFrZyBSsHiRWQU8Dtgraq6z0Xv7ze4c3JEpIKs8F+p6hvR6oJq5QzqnBzJpjC+DpxW1Z/lmAZ9rZxCruqeAr4JHBeR96N160lYK2cgIb5EoL5m9uzZevjw4fhAPDmyS5cu9fqtr6/3mY+4Hhxq+gwviLdKEG8V0+JTvXt76tQp5+VnQ0ODs9+uXbu8fn318MLdWwdBvFWCeKuYFp/qVZ2IODfmSzE9d+6c16+vCnpjY2O4qosjiLdKEG8V0+JTvaqrrKxk2rRpsbbjx487++V7IsKKFSuctsbGRqfN9MgH8VYJ4q0SxPsQkRoR+ZuInBaRkyLy3Wj9SyJySUTej15LSh9u35L3kjb67b1aVRtFZDRwhGwKyjNAu6r+pNCNZTIZdeXRLlu2zNnvrbfe8vodMWKE09bU1OS8pC2kcsIVshVRUNWbItKdkzPoKSYnBwZ5rZxicnIKqpWTm5PT2dnZByH3HYlzcgqtlZObk+N7SlF/kDgnZyjUyikmJ+f5JLVyBhKFHO0PAnEJM70uAzd27FgWL14ca9u+fbuzX1mZfwf1XdI2NTW5/Xq9DnGCeKsE8VYxLT7Vu7dVVVWsXr061uaqkwfw8MMPe/2OHDkyUTymRz6It0oQbxXT4lPNyamoqNDx4+PvdtXV1Tn7+X5sBP9U57uBaXrkg3irBPFWCeKtkuolrarS0dERa5s7d66z35EjR7x+k/5B2fTIB/FWCeKtYlp82n8zaQU+yFk1EfA/aLowfH6mqmpVbDxpir9v4yKHXdfaafgxvdsH8f3I5v7006/f+f6mv0e+Xym5eBFZJCL/EpFmEbmvbJyIZERkd2R/L0p07NkmNv+3R5v5InIjJxf4h3mDU9WSvYBy4CwwHRgONAGP9WizGtgULT8H7I7xUw18IVoeDfw7xs984I+9ia/UI/8k0Kyq51T1E2AX2Tp6ueTW1dsLLJAe5ZI8NfmKotTiJwMXcz63cH/Qn7ZR1Q7gBjDB5TAm/zeXL4lIk4j8SURm5guu1Hdy4vL3ek4vhbTJNvTX5GskeyrbHuX+/x6o9QVX6pFvAWpyPj8CXHa1EZFhwFhiHgXrqMn3Karapqrt0fLbQIWITPQFV2rxh4BaEXlURIaTPaDt69Emt67ecuCv2uPkw1OTL7fNpO5jhYg8SVbbf7zRlfJoH2lYQvbofBb4QbTux8BXo+VKYA/QDPwTmB7jYx7Zr8Ix4P3otQR4EXgxarMGOEl2RvkHMDdfbOEMzypBvFWCeKsE8VYxLf5/uUAyg4VVKI8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(agg_mask_norm, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('bertenv': conda)",
   "language": "python",
   "name": "python37464bitbertenvcondabe30b9b65ccd476ab0afdaa5ea99afd8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
