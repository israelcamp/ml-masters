{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, auc, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpack.utils import to_device, to_fp16\n",
    "from mlpack.trainer import TrainArgs, BaseTrainer\n",
    "from mlpack.imbalanced_sampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2014-01-05 00:00:00</th>\n",
       "      <th>2014-01-06 00:00:00</th>\n",
       "      <th>2014-01-07 00:00:00</th>\n",
       "      <th>2014-01-08 00:00:00</th>\n",
       "      <th>2014-01-09 00:00:00</th>\n",
       "      <th>2014-01-10 00:00:00</th>\n",
       "      <th>2014-01-11 00:00:00</th>\n",
       "      <th>2014-01-12 00:00:00</th>\n",
       "      <th>2014-01-13 00:00:00</th>\n",
       "      <th>2014-01-14 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-10-22 00:00:00</th>\n",
       "      <th>2016-10-23 00:00:00</th>\n",
       "      <th>2016-10-24 00:00:00</th>\n",
       "      <th>2016-10-25 00:00:00</th>\n",
       "      <th>2016-10-26 00:00:00</th>\n",
       "      <th>2016-10-27 00:00:00</th>\n",
       "      <th>2016-10-28 00:00:00</th>\n",
       "      <th>2016-10-29 00:00:00</th>\n",
       "      <th>2016-10-30 00:00:00</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONS_NO</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0387DD8A07E07FDA6271170F86AD9151</td>\n",
       "      <td>-0.828836</td>\n",
       "      <td>-0.824899</td>\n",
       "      <td>-0.815932</td>\n",
       "      <td>-0.822209</td>\n",
       "      <td>-0.807259</td>\n",
       "      <td>-0.791015</td>\n",
       "      <td>-0.794555</td>\n",
       "      <td>-0.817321</td>\n",
       "      <td>-0.826747</td>\n",
       "      <td>-0.812172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516633</td>\n",
       "      <td>0.582324</td>\n",
       "      <td>0.689155</td>\n",
       "      <td>0.788221</td>\n",
       "      <td>0.332229</td>\n",
       "      <td>0.657189</td>\n",
       "      <td>0.689740</td>\n",
       "      <td>0.604190</td>\n",
       "      <td>0.789638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>01D6177B5D4FFE0CABA9EF17DAFC2B84</td>\n",
       "      <td>-0.828836</td>\n",
       "      <td>-0.824899</td>\n",
       "      <td>-0.815932</td>\n",
       "      <td>-0.822209</td>\n",
       "      <td>-0.807259</td>\n",
       "      <td>-0.791015</td>\n",
       "      <td>-0.794555</td>\n",
       "      <td>-0.817321</td>\n",
       "      <td>-0.826747</td>\n",
       "      <td>-0.812172</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.459474</td>\n",
       "      <td>-1.458461</td>\n",
       "      <td>-1.463758</td>\n",
       "      <td>-1.465554</td>\n",
       "      <td>-1.466755</td>\n",
       "      <td>-1.466997</td>\n",
       "      <td>-1.468293</td>\n",
       "      <td>-1.463369</td>\n",
       "      <td>-1.468039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4B75AC4F2D8434CFF62DB64D0BB43103</td>\n",
       "      <td>-0.828836</td>\n",
       "      <td>-0.824899</td>\n",
       "      <td>-0.815932</td>\n",
       "      <td>-0.822209</td>\n",
       "      <td>-0.807259</td>\n",
       "      <td>-0.791015</td>\n",
       "      <td>-0.794555</td>\n",
       "      <td>-0.817321</td>\n",
       "      <td>-0.826747</td>\n",
       "      <td>-0.812172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>1.195724</td>\n",
       "      <td>1.314021</td>\n",
       "      <td>1.253836</td>\n",
       "      <td>1.377947</td>\n",
       "      <td>1.252855</td>\n",
       "      <td>1.216697</td>\n",
       "      <td>1.209608</td>\n",
       "      <td>1.144558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B32AC8CC6D5D805AC053557AB05F5343</td>\n",
       "      <td>-0.828836</td>\n",
       "      <td>-0.824899</td>\n",
       "      <td>-0.815932</td>\n",
       "      <td>-0.822209</td>\n",
       "      <td>-0.807259</td>\n",
       "      <td>-0.791015</td>\n",
       "      <td>-0.794555</td>\n",
       "      <td>-0.817321</td>\n",
       "      <td>-0.826747</td>\n",
       "      <td>-0.812172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960309</td>\n",
       "      <td>1.063827</td>\n",
       "      <td>1.288434</td>\n",
       "      <td>1.137711</td>\n",
       "      <td>1.309050</td>\n",
       "      <td>1.317606</td>\n",
       "      <td>1.191288</td>\n",
       "      <td>1.343474</td>\n",
       "      <td>1.159274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>EDFC78B07BA2908B3395C4EB2304665E</td>\n",
       "      <td>0.974797</td>\n",
       "      <td>1.189786</td>\n",
       "      <td>1.165372</td>\n",
       "      <td>1.030638</td>\n",
       "      <td>1.032385</td>\n",
       "      <td>1.057883</td>\n",
       "      <td>1.100623</td>\n",
       "      <td>1.119949</td>\n",
       "      <td>0.982739</td>\n",
       "      <td>1.092215</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040139</td>\n",
       "      <td>0.762741</td>\n",
       "      <td>0.725347</td>\n",
       "      <td>0.443744</td>\n",
       "      <td>0.413011</td>\n",
       "      <td>0.458308</td>\n",
       "      <td>0.528660</td>\n",
       "      <td>0.893334</td>\n",
       "      <td>0.849060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1030 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  2014-01-05 00:00:00  2014-01-06 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151            -0.828836            -0.824899   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -0.828836            -0.824899   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103            -0.828836            -0.824899   \n",
       "B32AC8CC6D5D805AC053557AB05F5343            -0.828836            -0.824899   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             0.974797             1.189786   \n",
       "\n",
       "                                  2014-01-07 00:00:00  2014-01-08 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151            -0.815932            -0.822209   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -0.815932            -0.822209   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103            -0.815932            -0.822209   \n",
       "B32AC8CC6D5D805AC053557AB05F5343            -0.815932            -0.822209   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             1.165372             1.030638   \n",
       "\n",
       "                                  2014-01-09 00:00:00  2014-01-10 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151            -0.807259            -0.791015   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -0.807259            -0.791015   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103            -0.807259            -0.791015   \n",
       "B32AC8CC6D5D805AC053557AB05F5343            -0.807259            -0.791015   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             1.032385             1.057883   \n",
       "\n",
       "                                  2014-01-11 00:00:00  2014-01-12 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151            -0.794555            -0.817321   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -0.794555            -0.817321   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103            -0.794555            -0.817321   \n",
       "B32AC8CC6D5D805AC053557AB05F5343            -0.794555            -0.817321   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             1.100623             1.119949   \n",
       "\n",
       "                                  2014-01-13 00:00:00  2014-01-14 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151            -0.826747            -0.812172   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -0.826747            -0.812172   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103            -0.826747            -0.812172   \n",
       "B32AC8CC6D5D805AC053557AB05F5343            -0.826747            -0.812172   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             0.982739             1.092215   \n",
       "\n",
       "                                  ...  2016-10-22 00:00:00  \\\n",
       "CONS_NO                           ...                        \n",
       "0387DD8A07E07FDA6271170F86AD9151  ...             0.516633   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84  ...            -1.459474   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103  ...             0.839572   \n",
       "B32AC8CC6D5D805AC053557AB05F5343  ...             0.960309   \n",
       "EDFC78B07BA2908B3395C4EB2304665E  ...             1.040139   \n",
       "\n",
       "                                  2016-10-23 00:00:00  2016-10-24 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151             0.582324             0.689155   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -1.458461            -1.463758   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103             1.195724             1.314021   \n",
       "B32AC8CC6D5D805AC053557AB05F5343             1.063827             1.288434   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             0.762741             0.725347   \n",
       "\n",
       "                                  2016-10-25 00:00:00  2016-10-26 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151             0.788221             0.332229   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -1.465554            -1.466755   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103             1.253836             1.377947   \n",
       "B32AC8CC6D5D805AC053557AB05F5343             1.137711             1.309050   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             0.443744             0.413011   \n",
       "\n",
       "                                  2016-10-27 00:00:00  2016-10-28 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151             0.657189             0.689740   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -1.466997            -1.468293   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103             1.252855             1.216697   \n",
       "B32AC8CC6D5D805AC053557AB05F5343             1.317606             1.191288   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             0.458308             0.528660   \n",
       "\n",
       "                                  2016-10-29 00:00:00  2016-10-30 00:00:00  \\\n",
       "CONS_NO                                                                      \n",
       "0387DD8A07E07FDA6271170F86AD9151             0.604190             0.789638   \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84            -1.463369            -1.468039   \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103             1.209608             1.144558   \n",
       "B32AC8CC6D5D805AC053557AB05F5343             1.343474             1.159274   \n",
       "EDFC78B07BA2908B3395C4EB2304665E             0.893334             0.849060   \n",
       "\n",
       "                                  flag  \n",
       "CONS_NO                                 \n",
       "0387DD8A07E07FDA6271170F86AD9151     1  \n",
       "01D6177B5D4FFE0CABA9EF17DAFC2B84     1  \n",
       "4B75AC4F2D8434CFF62DB64D0BB43103     1  \n",
       "B32AC8CC6D5D805AC053557AB05F5343     1  \n",
       "EDFC78B07BA2908B3395C4EB2304665E     1  \n",
       "\n",
       "[5 rows x 1030 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/ENEL/df_BoxCox.csv', sep='\\t', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, random_state=1, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df_train.values\n",
    "data_valid = df_valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = data_train[:,:-1]\n",
    "y_train = data_train[:,-1]\n",
    "\n",
    "x_valid = data_valid[:,:-1]\n",
    "y_valid = data_valid[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.concatenate(([2*[0] for _ in range(len(x_train))], x_train), axis=1)\n",
    "# x_valid = np.concatenate(([2*[0] for _ in range(len(x_valid))], x_valid), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21186, 1029), (21186, 1029))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(len(x_train), -1, 7)\n",
    "x_train = np.expand_dims(x_train, 1)\n",
    "\n",
    "x_valid = x_valid.reshape(len(x_valid), -1, 7)\n",
    "x_valid = np.expand_dims(x_valid, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.transpose(0, 1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = x_valid.transpose(0, 1, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21186, 21186)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_valid), len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([0., 1.]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENELDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.x[idx], self.y[idx]        \n",
    "        return x.astype(np.float32), y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ENELDataset(x_train, y_train)\n",
    "ds_valid = ENELDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySampler(ImbalancedDatasetSampler):\n",
    "    \n",
    "    def _get_label(self, dataset, idx):\n",
    "        _, y = dataset[idx]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, \n",
    "                      sampler=MySampler(ds_train),\n",
    "                      batch_size=64, pin_memory=True, num_workers=8)\n",
    "dl_valid = DataLoader(ds_valid,\n",
    "                      batch_size=64, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    r\"\"\"Applies the Gaussian Error Linear Units function:\n",
    "    .. math::\n",
    "        \\text{GELU}(x) = x * \\Phi(x)\n",
    "    where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Output: :math:`(N, *)`, same shape as the input\n",
    "    .. image:: scripts/activation_images/GELU.png\n",
    "    Examples::\n",
    "        >>> m = nn.GELU()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        return F.gelu(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query = nn.Conv2d(*args, **kwargs)\n",
    "        self.key = nn.Conv2d(*args, **kwargs)\n",
    "        self.value = nn.Conv2d(*args, **kwargs)\n",
    "        \n",
    "        self.d = self.query.out_channels**(0.5)\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        query = self.query(*args, **kwargs)\n",
    "        key = self.key(*args, **kwargs)\n",
    "        value = self.value(*args, **kwargs)\n",
    "        \n",
    "        attention = torch.matmul(query, key.transpose(-2, -1))\n",
    "        attention = attention / self.d\n",
    "        attention = nn.Softmax(dim=-1)(attention)\n",
    "        attention = torch.matmul(attention, value)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySelfConvModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self.block(1, 90),\n",
    "            self.block(90,90),\n",
    "            self.block(90,90),\n",
    "            self.block(90,90),\n",
    "#             self.block(90,90),\n",
    "            nn.MaxPool2d(3)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(90 * 2 * 49, 90),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(90,2)\n",
    "        )\n",
    "        \n",
    "    def block(self, in_channels, out_channels, dropout=0.5):\n",
    "        return nn.Sequential(\n",
    "            SelfAttentionConv(in_channels, out_channels, kernel_size=3, padding=1),  # n x out_channels x m x out_features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o = self.net(x)\n",
    "        o = o.view(x.shape[0], -1)\n",
    "        o = self.classifier(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = MySelfConvModel()\n",
    "        self.wide = WideLinear()\n",
    "        self.classifier = nn.Linear(60 + 90, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        wide = self.wide(x)\n",
    "        o = torch.cat((wide, conv), dim=1)\n",
    "        o = self.dropout(o)\n",
    "        o = self.classifier(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedConv(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv_block(1, 90),\n",
    "#             self.conv_block(90,90),\n",
    "            self.conv_block(90,90),\n",
    "            self.attention_block(90,90),\n",
    "            nn.MaxPool2d(3)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(90 * 2 * 49, 90),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(90,2)\n",
    "        )\n",
    "        \n",
    "    def attention_block(self, in_channels, out_channels, dropout=0.4):\n",
    "        return nn.Sequential(\n",
    "            SelfAttentionConv(in_channels, out_channels, kernel_size=3, padding=1),  # n x out_channels x m x out_features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels, dropout=0.4):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # n x out_channels x m x out_features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o = self.net(x)\n",
    "        o = o.view(x.shape[0], -1)\n",
    "        o = self.classifier(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MySelfConvModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySelfConvModel(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): SelfAttentionConv(\n",
       "        (query): Conv2d(1, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (key): Conv2d(1, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (value): Conv2d(1, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SelfAttentionConv(\n",
       "        (query): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (key): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (value): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): SelfAttentionConv(\n",
       "        (query): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (key): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (value): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SelfAttentionConv(\n",
       "        (query): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (key): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (value): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (4): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=8820, out_features=90, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=90, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    o = model(x.to(device))\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision_at_k(y_true,class_probs,k,threshold=0.5,class_of_interest=1,isSorted=False):\n",
    "\n",
    "    if (not isSorted):\n",
    "\n",
    "        # Selecting the probs on the class of interest (coi)\n",
    "        coi_probs = class_probs[:,class_of_interest]\n",
    "\n",
    "        # print('Selecionando as probs da classe de interesse:',class_of_interest)\n",
    "        # print(coi_probs)\n",
    "\n",
    "        # print('Ordem de entrada dos targets')\n",
    "        # print(y_true)\n",
    "\n",
    "        # Sorting the coi probs and selecting the top k\n",
    "        # print('Ordenando as probs e os targets correspondentes:')\n",
    "        sorted_coi_probs = np.sort(coi_probs)[::-1]\n",
    "        sorted_y = y_true[np.argsort(coi_probs)[::-1]]\n",
    "        \n",
    "        # Calculating the precision for \n",
    "        # print(sorted_coi_probs)\n",
    "        # print(sorted_y)\n",
    "\n",
    "    else:\n",
    "        # Code otimization so the algorithm doesn't have to sort the data twice\n",
    "        sorted_coi_probs = class_probs\n",
    "        sorted_y = y_true\n",
    "\n",
    "\n",
    "    # Selecting the top k probs and their respective labels\n",
    "    sorted_coi_probs = sorted_coi_probs[:k]\n",
    "    sorted_y = sorted_y[:k]\n",
    "\n",
    "    # print('Selecionando as top',k,'probs')\n",
    "    # print(sorted_coi_probs)\n",
    "    # print(sorted_y)\n",
    "\n",
    "    # Atributing class based on the threshold\n",
    "    sorted_predicted_classes = np.where(sorted_coi_probs>threshold,\n",
    "                                        float(class_of_interest),\n",
    "                                        0.0)\n",
    "\n",
    "    # print('Atribuindo a classe baseada no threshold de ',threshold)\n",
    "#     print(sorted_predicted_classes)\n",
    "\n",
    "    # print('PrecisÃ£o em K:')\n",
    "\n",
    "    # print(k)\n",
    "\n",
    "    precisionK = np.sum(sorted_predicted_classes == sorted_y)/k  \n",
    "\n",
    "    return precisionK\n",
    "\n",
    "def map_at_N(y_true,class_probs,N,thrs=0.5,class_of_interest=1):\n",
    "\n",
    "    # Calls the precision at k function with the values already sorted and calculates the average precision\n",
    "    # weighted by the number of positive classes inside the sample\n",
    "\n",
    "    # Declaring the list to hold the precisions\n",
    "    pks = []\n",
    "\n",
    "    # Selecting the probs on the class of interest (coi)\n",
    "    coi_probs = class_probs[:,class_of_interest]\n",
    "\n",
    "    # Sorting the class of interest and its respective label\n",
    "    sorted_coi_probs = np.sort(coi_probs)[::-1]\n",
    "    sorted_y = y_true[np.argsort(coi_probs)[::-1]]\n",
    "\n",
    "    # Selecting the top N scores \n",
    "    sorted_coi_probs = sorted_coi_probs[:N]\n",
    "    sorted_y = sorted_y[:N]\n",
    "\n",
    "\n",
    "    ## TESTANDO APENAS ##\n",
    "    sorted_y[-1] = 1 \n",
    "\n",
    "    # Identifying the positions of the class of interest inside the top N\n",
    "    top_coi_indexes = np.argwhere(sorted_y>0)\n",
    "\n",
    "    for value in top_coi_indexes:\n",
    "        \n",
    "        # Adjusting the index\n",
    "        limite = value[0] + 1\n",
    "        \n",
    "        pks.append(\n",
    "                    precision_at_k(sorted_y[:limite],\n",
    "                    sorted_coi_probs[:limite],\n",
    "                    limite,threshold=thrs,isSorted=True)\n",
    "                    )\n",
    "\n",
    "    pks = np.array(pks)\n",
    "    \n",
    "    # print(pks)\n",
    "    # print(sorted_coi_probs)\n",
    "    # print(sorted_y)\n",
    "    return pks.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(BaseTrainer):\n",
    "    \n",
    "    @staticmethod\n",
    "    def dataloader_generator(dataloader):\n",
    "        for x, y in dataloader:\n",
    "#             x = x.unsqueeze(-1)\n",
    "            x, y = to_device(x, y, device=device)\n",
    "            yield {\n",
    "                'inputs': {\n",
    "                    'x': x\n",
    "                },\n",
    "                'targets': {\n",
    "                    'y': y\n",
    "                }\n",
    "            }\n",
    "        \n",
    "    @staticmethod\n",
    "    def loss_from_model(model_output, targets, loss_fn=None):\n",
    "        y = targets['y']\n",
    "        return loss_fn(model_output, y)\n",
    "    \n",
    "    def evaluate_fn(self, model, dataloader, loss_fn):\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "        probs = []\n",
    "        dl_gen = self.dataloader_generator(dataloader)\n",
    "        for batch in self.tqdm(dl_gen, leave=False, desc='Eval...', total=len(dataloader)):\n",
    "            inputs = batch['inputs']\n",
    "            targets = batch['targets']\n",
    "\n",
    "            with torch.no_grad():\n",
    "                o = model(**inputs)\n",
    "\n",
    "            loss = self.loss_from_model(o, targets, loss_fn)\n",
    "\n",
    "            y = targets['y']\n",
    "            \n",
    "            probs += torch.softmax(o, dim=-1).detach().cpu().numpy().tolist()\n",
    "            preds += o.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += y.detach().cpu().numpy().tolist()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        acc = accuracy_score(trues, preds)\n",
    "        f1 = f1_score(trues, preds)\n",
    "        conf = confusion_matrix(trues, preds)\n",
    "        map100 = map_at_N(np.array(trues), np.array(probs), 100)\n",
    "        map200 = map_at_N(np.array(trues), np.array(probs), 200)\n",
    "#         return trues, preds, probs\n",
    "        roc = roc_auc_score(np.array(trues), np.array(probs)[:,1])\n",
    "\n",
    "        print('--- Validation ---')\n",
    "        print(f'F1 = {f1}\\t Acc = {acc}')\n",
    "        print(f'MAP@100 = {map100}')\n",
    "        print(f'MAP@200 = {map200}')\n",
    "        print(f'ROC = {roc}')\n",
    "        print(conf)\n",
    "        return np.array(losses).mean(), map100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\n",
    "                \"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\n",
    "                \"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[\n",
    "                        [None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * \\\n",
    "                        state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
    "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay']\n",
    "                                         * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size *\n",
    "                                         group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay']\n",
    "                                         * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(torch.tensor([1., 1.]).to(device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=2e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, weight_decay=2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, 5e-4, 1e-3, step_size_up=3*len(dl_train), cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainArgs(100, 'enel_selfatt_conv_boxcox_split80.ckp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(grad_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('enel_selfatt_conv_boxcox_split50.ckp'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.load_state_dict(torch.load('enel_selfatt_conv_10_boxcox_optimizer.ckp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval...', max=332, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation ---\n",
      "F1 = 0.4344885883347422\t Acc = 0.905267629566695\n",
      "MAP@100 = 0.9919810663885326\n",
      "MAP@200 = 0.9625359408012584\n",
      "ROC = 0.7888609094529886\n",
      "[[18408   952]\n",
      " [ 1055   771]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.30554942591452455, 0.9919810663885326)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate_fn(model, dl_valid, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129b6eac16d240ef9c337fdc6ed866b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training...', style=ProgressStyle(description_width='initial'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train Loss 0.5963239251442676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval...', max=133, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation ---\n",
      "F1 = 0.30082543564659125\t Acc = 0.7301474926253687\n",
      "MAP@100 = 0.7369067196199709\n",
      "MAP@200 = 0.6735970384084653\n",
      "ROC = 0.7750521960837545\n",
      "[[5696 2040]\n",
      " [ 247  492]]\n",
      "---Valid\n",
      "Loss 0.6112719352980306\n",
      "Metric 0.7369067196199709\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train Loss 0.5370396554470063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval...', max=133, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation ---\n",
      "F1 = 0.34945631796025495\t Acc = 0.7952802359882006\n",
      "MAP@100 = 0.8358511404922284\n",
      "MAP@200 = 0.7831746323508135\n",
      "ROC = 0.7939919054089416\n",
      "[[6274 1462]\n",
      " [ 273  466]]\n",
      "---Valid\n",
      "Loss 0.5396848748949238\n",
      "Metric 0.8358511404922284\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train Loss 0.5029909178895771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval...', max=133, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation ---\n",
      "F1 = 0.37488869100623323\t Acc = 0.8343362831858407\n",
      "MAP@100 = 0.8467411214210699\n",
      "MAP@200 = 0.8014969838286127\n",
      "ROC = 0.8007679506250237\n",
      "[[6650 1086]\n",
      " [ 318  421]]\n",
      "---Valid\n",
      "Loss 0.47484045884663\n",
      "Metric 0.8467411214210699\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train Loss 0.47330415732455705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval...', max=133, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation ---\n",
      "F1 = 0.377814845704754\t Acc = 0.823952802359882\n",
      "MAP@100 = 0.8665521766776687\n",
      "MAP@200 = 0.8264995672785402\n",
      "ROC = 0.807388492092923\n",
      "[[6530 1206]\n",
      " [ 286  453]]\n",
      "---Valid\n",
      "Loss 0.4362959982757282\n",
      "Metric 0.8665521766776687\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train Loss 0.42448259355886925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval...', max=133, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation ---\n",
      "F1 = 0.37835534725181086\t Acc = 0.8278466076696165\n",
      "MAP@100 = 0.871309688653996\n",
      "MAP@200 = 0.834986385521709\n",
      "ROC = 0.8136115281977797\n",
      "[[6572 1164]\n",
      " [ 295  444]]\n",
      "---Valid\n",
      "Loss 0.43927088379859924\n",
      "Metric 0.871309688653996\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train Loss 0.39533989496950833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Eval...', max=133, style=ProgressStyle(description_width='iniâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validation ---\n",
      "F1 = 0.4154447702834799\t Acc = 0.8588790560471976\n",
      "MAP@100 = 0.9054059952239062\n",
      "MAP@200 = 0.8646331665164813\n",
      "ROC = 0.8075156588251263\n",
      "[[6854  882]\n",
      " [ 314  425]]\n",
      "---Valid\n",
      "Loss 0.382755560534341\n",
      "Metric 0.9054059952239062\n",
      "Saved new checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf86d7c7c0dd413ebb80ab7559113f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-dacc2cdd3046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Mestrado/ml-masters/mlpack/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, args, model, dl_train, dl_valid, optimizer, loss_fn, scheduler)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     torch.nn.utils.clip_grad_norm_(\n\u001b[0;32m--> 117\u001b[0;31m                         model.parameters(), self.max_grad_norm)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpenv/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(args, model, dl_train, dl_valid, optimizer, loss_fn, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_nlpenv)",
   "language": "python",
   "name": "conda_nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
